<?xml version="1.0" encoding="utf-8"?>
<search>
    
    
    <entry>
        <title><![CDATA[Spark聚合操作]]></title>
        <url>http://honeyyouyou.com/2019/11/19/spark_aggregate_operations/</url>
        <content type="html"><![CDATA[<h1 id="Spark聚合操作-Spark-Aggregate-operations"><a href="#Spark聚合操作-Spark-Aggregate-operations" class="headerlink" title="Spark聚合操作 Spark Aggregate operations"></a>Spark聚合操作 Spark Aggregate operations</h1><h2 id="聚合操作简介"><a href="#聚合操作简介" class="headerlink" title="聚合操作简介"></a>聚合操作简介</h2><p>聚合操作，在sql，在编程概念中，或者说在人类思想概念中都是一个很重要的东西。<br>这个思想从哪里来的呢？应该是类推，演绎的继续。我们拆箱子，想看看每个箱子里面都多少个玩具，然后想知道每个玩具里面有多少个零件，想知道每个零件有几个齿轮轮毂，每个轮毂有多少道痕，每个痕里有几个细菌，每个细菌中有多少分子，到原子，到夸克就到了我们人类知道的最小的粒子了（不算时间，能量能虚幻之物）。然后往上，我们可以回滚。也就有了基本的聚合概念。每个屋里有多少箱子，每个城市有多少个屋，每个国家多少个城市，每个星球多少个国家，每个星系多少个星球，每个宇宙多少个星系。<br>还是回到讨论的聚合概念中来。有了刚才的类比，就比较好想聚合的概念了。在手中拥有的数据中，可以通过任意维度进行聚合，聚合后又可以查询原始数据。这就是作为的上钻和下钻的概念。<br>在sql中如何进行上下钻，切片等操作呢？</p>
<h2 id="Spark中上卷操作"><a href="#Spark中上卷操作" class="headerlink" title="Spark中上卷操作"></a>Spark中上卷操作</h2><p>这个需要分RDD和Dataframe来讲，因为这个的存储方式不一样，所以需要分开讲。</p>
<h3 id="Spark中RDD的上卷"><a href="#Spark中RDD的上卷" class="headerlink" title="Spark中RDD的上卷"></a>Spark中RDD的上卷</h3><p>RDD中首先将数据分为两类，list类型的map类型的。可以理解为一维和二维的数据。</p>
<h4 id="一维数据的操作"><a href="#一维数据的操作" class="headerlink" title="一维数据的操作"></a>一维数据的操作</h4><p>RDD中一维数据的上卷操作主要有一个。</p>
<ul>
<li><p>reduce<br>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.sparkContext.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b)</span><br><span class="line"><span class="keyword">val</span> result:<span class="type">Int</span> = rdd.reduce(func)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>结果：6</p>
</li>
<li><p>aggregate<br>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.sparkContext.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b)</span><br><span class="line"><span class="keyword">val</span> result:<span class="type">Int</span> = rdd.aggregate(<span class="number">0</span>)(func,func)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>结果：6</p>
</li>
</ul>
<h4 id="二维数据的操作"><a href="#二维数据的操作" class="headerlink" title="二维数据的操作"></a>二维数据的操作</h4><p>RDD中二维数据的上卷操作主要有以下几个。</p>
<ol>
<li>reduceByKey<br>此处举两个例子：</li>
</ol>
<ul>
<li><p>第一个使用网上的例子</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.sparkContext.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">2</span>),(<span class="number">3</span>,<span class="number">4</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; &#123;a+b&#125;</span><br><span class="line"><span class="keyword">val</span> result = rdd.reduceByKey(func)</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<p>结果: (1,2),(3,10)</p>
</li>
<li><p>第二个使用经典的例子，wordcount<br>写测试文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -e <span class="string">"a b c\nb a c\na d c b e\nb c a d\nc e f f"</span> &gt; /tmp/wc.log</span><br></pre></td></tr></table></figure>
<p>统计分析</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; &#123;a+b&#125;</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).reduceByKey(func)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>扩展 WordCount使用不同的聚合函数</p>
<ol>
<li>groupByKey<br>直接使用groupByKey的wordcount例子。Dataset版。<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>)</span><br><span class="line"><span class="keyword">val</span> func = (a:(<span class="type">String</span>,<span class="type">Int</span>),b:(<span class="type">String</span>,<span class="type">Int</span>)) =&gt; (a._1,a._2+b._2)</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).groupByKey(_._1).reduceGroups(func).map(_._2)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>直接使用groupByKey的wordcount例子。RDD版。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="keyword">val</span> func = (a:(<span class="type">String</span>,<span class="type">Int</span>),b:(<span class="type">String</span>,<span class="type">Int</span>)) =&gt; (a._1,a._2+b._2)</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).groupByKey(_._1).reduceGroups(func).map(_._2)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>
<p>直接使用groupByKey的wordcount例子。RDD获取长度版。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).groupByKey().mapValues(_.size)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><p>foldByKey</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; &#123;a+b&#125;</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).foldByKey(<span class="number">0</span>)(func)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>
</li>
<li><p>aggregateByKey</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; &#123;a+b&#125;</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).aggregateByKey(<span class="number">0</span>)(func,func)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>
</li>
<li><p>combineByKey</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createCombiner</span> </span>= (x : <span class="type">Int</span>) =&gt; x</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergeValue</span> </span>= (acc1: <span class="type">Int</span>, x:<span class="type">Int</span> ) =&gt; acc1 + x</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergeCombiner</span> </span>= (acc1:<span class="type">Int</span>, acc2:<span class="type">Int</span>) =&gt; acc1 + acc2</span><br><span class="line"><span class="comment">//val result = rdd.flatMap( line =&gt; &#123;for(ele &lt;- line.split(" ")) yield (ele,1)&#125;).combineByKey( (x:Int) =&gt; x  , (acc1:Int, x:Int ) =&gt; acc1 + x  , ( acc1:Int , acc2 :Int ) =&gt; acc1 + acc2  )</span></span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).combineByKey(createCombiner,mergeValue,mergeCombiner)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="Spark中DataFrame的上卷"><a href="#Spark中DataFrame的上卷" class="headerlink" title="Spark中DataFrame的上卷"></a>Spark中DataFrame的上卷</h3><p>DataFrame可以认为是包含Schema的RDD。它的上卷没有一维二维之分。</p>
<h4 id="GroupBy"><a href="#GroupBy" class="headerlink" title="GroupBy"></a>GroupBy</h4><ol>
<li><p>Build df example method1</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(</span><br><span class="line">  <span class="type">Seq</span>(</span><br><span class="line">    (<span class="string">"first"</span>, <span class="type">Array</span>(<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">2.1</span>, <span class="number">5.4</span>)),</span><br><span class="line">    (<span class="string">"test"</span>, <span class="type">Array</span>(<span class="number">1.5</span>, <span class="number">0.5</span>, <span class="number">0.9</span>, <span class="number">3.7</span>)),</span><br><span class="line">    (<span class="string">"choose"</span>, <span class="type">Array</span>(<span class="number">8.0</span>, <span class="number">2.9</span>, <span class="number">9.1</span>, <span class="number">2.5</span>))</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> dfWithoutSchema = spark.createDataFrame(rdd)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Build df example method2</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(</span><br><span class="line">  <span class="type">Seq</span>(</span><br><span class="line">    (<span class="string">"first"</span>, <span class="type">Array</span>(<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">2.1</span>, <span class="number">5.4</span>)),</span><br><span class="line">    (<span class="string">"test"</span>, <span class="type">Array</span>(<span class="number">1.5</span>, <span class="number">0.5</span>, <span class="number">0.9</span>, <span class="number">3.7</span>)),</span><br><span class="line">    (<span class="string">"choose"</span>, <span class="type">Array</span>(<span class="number">8.0</span>, <span class="number">2.9</span>, <span class="number">9.1</span>, <span class="number">2.5</span>))</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> dfWithSchema = spark.createDataFrame(rdd).toDF(<span class="string">"id"</span>, <span class="string">"vals"</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>Build df example method3</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">val</span> rowsRdd: <span class="type">RDD</span>[<span class="type">Row</span>] = sc.parallelize(</span><br><span class="line">  <span class="type">Seq</span>(</span><br><span class="line">    <span class="type">Row</span>(<span class="string">"first"</span>, <span class="number">2.0</span>, <span class="number">7.0</span>),</span><br><span class="line">    <span class="type">Row</span>(<span class="string">"second"</span>, <span class="number">3.5</span>, <span class="number">2.5</span>),</span><br><span class="line">    <span class="type">Row</span>(<span class="string">"third"</span>, <span class="number">7.0</span>, <span class="number">5.9</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> schema = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)).add(<span class="type">StructField</span>(<span class="string">"val1"</span>, <span class="type">DoubleType</span>, <span class="literal">true</span>)).add(<span class="type">StructField</span>(<span class="string">"val2"</span>, <span class="type">DoubleType</span>, <span class="literal">true</span>))</span><br><span class="line"><span class="keyword">val</span> df = spark.createDataFrame(rowsRdd, schema)</span><br></pre></td></tr></table></figure></li>
<li><p>Build df example method4</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ELE</span>(<span class="params">id:<span class="type">String</span>,val1:<span class="type">Double</span>,val2:<span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rdd</span> </span>= sc.parallelize(</span><br><span class="line">  <span class="type">Seq</span>(</span><br><span class="line">    (<span class="string">"first"</span>, <span class="number">2.0</span>, <span class="number">7.0</span>),</span><br><span class="line">    (<span class="string">"second"</span>, <span class="number">3.5</span>, <span class="number">2.5</span>),</span><br><span class="line">    (<span class="string">"third"</span>, <span class="number">7.0</span>, <span class="number">5.9</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> df = rdd.map(s =&gt; <span class="type">ELE</span>(s._1,s._2,s._3)).toDF</span><br></pre></td></tr></table></figure>
</li>
<li><p>Do groupby on df</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.groupBy(<span class="string">"id"</span>).agg(sum(<span class="string">"val1"</span>).as(<span class="string">"total"</span>)).show(<span class="literal">false</span>)</span><br><span class="line">df.groupBy().agg(sum(<span class="string">"val1"</span>).as(<span class="string">"total"</span>)).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="Partition-By"><a href="#Partition-By" class="headerlink" title="Partition By"></a>Partition By</h4><ul>
<li>参考：<a href="https://stackoverflow.com/questions/36447057/spark-group-multiple-rdd-items-by-key" target="_blank" rel="noopener">ReduceByKeyAndGroupByKey</a></li>
<li>参考：<a href="https://gerardnico.com/db/spark/rdd/reduce" target="_blank" rel="noopener">ReduceAndReduceByKey</a></li>
<li>参考：<a href="https://blog.csdn.net/u013514928/article/details/56680825" target="_blank" rel="noopener">Aggregate与AggregateByKey</a></li>
<li>参考：<a href="https://backtobazics.com/big-data/spark/apache-spark-aggregatebykey-example/" target="_blank" rel="noopener">Aggregate实例</a></li>
<li>参考：<a href="https://medium.com/@yesilliali/apache-spark-understanding-zerovalue-in-aggregatebykey-function-3d7df62567ae" target="_blank" rel="noopener">理解AggregateByKey中的Zero值</a></li>
<li>参考：<a href="https://dev.to/yashwanth2804/different-ways-to-word-count-in-apache-spark-1193" target="_blank" rel="noopener">DifferentWaysToWordCountInApacheSpark</a></li>
<li>参考：<a href="https://github.com/vaquarkhan/Apache-Kafka-poc-and-notes/wiki/reducebykey-vs-combinebykey-Apache-Spark" target="_blank" rel="noopener">CombineByKey</a></li>
<li>参考：<a href="https://backtobazics.com/big-data/apache-spark-combinebykey-example/" target="_blank" rel="noopener">CombineByKey2</a></li>
<li>参考：<a href="https://docs.databricks.com/spark/latest/spark-sql/udaf-scala.html" target="_blank" rel="noopener">UDAF实例</a></li>
<li>参考：<a href="https://stackoverflow.com/questions/37440373/spark-dataframe-aggregate-column-values-by-key-into-list" target="_blank" rel="noopener">DataFrame中Concate操作</a></li>
<li>参考：<a href="https://stackoverflow.com/questions/29383578/how-to-convert-rdd-object-to-dataframe-in-spark" target="_blank" rel="noopener">RDDToDataframe</a></li>
<li>参考：<a href="https://indatalabs.com/blog/convert-spark-rdd-to-dataframe-dataset" target="_blank" rel="noopener">RDDToDFAndDataSet</a></li>
</ul>
]]></content>
        
        <categories>
            
            <category> learn </category>
            
            <category> principle </category>
            
            <category> spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 学习 </tag>
            
            <tag> 代码 </tag>
            
            <tag> Spark </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[Spark源码分析001]]></title>
        <url>http://honeyyouyou.com/2019/11/14/spark_analysis_001/</url>
        <content type="html"><![CDATA[<h1 id="SparkAnalysis001"><a href="#SparkAnalysis001" class="headerlink" title="SparkAnalysis001"></a>SparkAnalysis001</h1><h2 id="系统和软件的版本"><a href="#系统和软件的版本" class="headerlink" title="系统和软件的版本"></a>系统和软件的版本</h2><h3 id="Spark的版本"><a href="#Spark的版本" class="headerlink" title="Spark的版本"></a>Spark的版本</h3><p>在Spark<a href="http://spark.apache.org/" target="_blank" rel="noopener">官网</a>中，最新的发布版本是3.0Preview(Nov 06,2019)。最新的两个Release版本是2.3.4(Sep 09,2019)和2.4.4(Sep 01,2019)。而在Spark的<a href="https://github.com/apache/spark" target="_blank" rel="noopener">Github页面</a>中，最新的分支应该是master。而最新的release应该是branch-2.4。所以直接从Spark的github中的master分支开始分析。</p>
<h3 id="Java的版本"><a href="#Java的版本" class="headerlink" title="Java的版本"></a>Java的版本</h3><p>Java就使用oracle的jdk1.8。</p>
<h3 id="Scala的版本"><a href="#Scala的版本" class="headerlink" title="Scala的版本"></a>Scala的版本</h3><h2 id="代码在Github的链接和分支"><a href="#代码在Github的链接和分支" class="headerlink" title="代码在Github的链接和分支"></a>代码在Github的链接和分支</h2><h3 id="代码链接"><a href="#代码链接" class="headerlink" title="代码链接"></a>代码链接</h3><h3 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h3><h2 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h2><h3 id="core"><a href="#core" class="headerlink" title="core"></a>core</h3><h3 id="common"><a href="#common" class="headerlink" title="common"></a>common</h3><h2 id="测试环境搭建"><a href="#测试环境搭建" class="headerlink" title="测试环境搭建"></a>测试环境搭建</h2><h3 id="编译方法"><a href="#编译方法" class="headerlink" title="编译方法"></a>编译方法</h3><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
        
        <categories>
            
            <category> learn </category>
            
            <category> code </category>
            
            <category> spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 学习 </tag>
            
            <tag> 代码 </tag>
            
            <tag> Spark </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[如何迅速的学习新东西]]></title>
        <url>http://honeyyouyou.com/2019/11/01/how_to_learn_new_things/</url>
        <content type="html"><![CDATA[<h1 id="如何迅速的学习新东西"><a href="#如何迅速的学习新东西" class="headerlink" title="如何迅速的学习新东西"></a>如何迅速的学习新东西</h1><h2 id="学习是什么"><a href="#学习是什么" class="headerlink" title="学习是什么"></a>学习是什么</h2><p>这是一个很大的话题，也是一个很小的话题。大到可以说学习是人和其他动物的区别，小到每天看到一则新闻都是在学习。</p>
<h3 id="先从大处说"><a href="#先从大处说" class="headerlink" title="先从大处说"></a>先从大处说</h3><p>地球形成于约45亿年前．大约42亿年前海洋形成．大约35亿年前海洋生命出现。最早的人属约出现在230万到240万年前的非洲大陆。早期智人大约在25万到40万年前演变出来。人类能站在生物链的顶端的原因是什么呢？为什么不是恐龙称霸地球？为什么不是蚂蚁？不是蟑螂？不是鲸鱼？人类个头并不大，跑的也不快？靠的是什么呢？答案是想象能力，也可以叫八卦能力。也是今天讨论的学习能力的一种。所以说往大了说学习是人类称霸地球的重要原因，你说这个话题大否？</p>
<h3 id="再从小处说"><a href="#再从小处说" class="headerlink" title="再从小处说"></a>再从小处说</h3><p>你看了一则新闻，说哪里地震了，正好今天考试应用到自己的作文中。这是一种学习。早上上班的路上，突然想起昨天伙伴说附近有家早餐便宜又好吃，正好还没吃东西，就去他家。这是一种学习。学习是不是一个很小的话题？</p>
<p>维基百科给学习的定义是这样的：<strong>学习是透过外界教授或从自身经验提高能力的过程。</strong>所以学习途径有两种，一种听说而来，从外界而来。一种是自己亲身经历。老师说，不要碰火，会很烫手。有些同学听话，相信老师，就不碰了。有些调皮大胆的，非要自己试试。烫了一个包，自己亲身学到了，火确实很烫手。然后他又告诉了同学。小胆的同学听到更不敢碰了。这个过程基本涵盖了两种学习方式。第一种靠的就是想象，第二种靠的是亲身体验。</p>
<h2 id="人为什么要学习？"><a href="#人为什么要学习？" class="headerlink" title="人为什么要学习？"></a>人为什么要学习？</h2><h3 id="学习是你安身立命之本"><a href="#学习是你安身立命之本" class="headerlink" title="学习是你安身立命之本"></a>学习是你安身立命之本</h3><p>往俗一点说，学习是你生活的资本．古人云：</p>
<blockquote>
<p>书中自有颜如玉，书中自有黄金屋．</p>
</blockquote>
<p>就是这个道理．尤其是现在的中国学习或读书是你跨越阶层最快的一条路．</p>
<h3 id="学习是你提升人生境界的方式"><a href="#学习是你提升人生境界的方式" class="headerlink" title="学习是你提升人生境界的方式"></a>学习是你提升人生境界的方式</h3><p>三字经云：</p>
<blockquote>
<p>人不学，不知义；玉不琢，不成器．    </p>
</blockquote>
<p>人通过学习达到自我提升的目的．每个人是有自己一定境界的，有的人境界高，成仙成佛成圣人，有些人温饱足矣．学习是你提升境界的重要方式．</p>
<h2 id="关于学习方法的几种观点"><a href="#关于学习方法的几种观点" class="headerlink" title="关于学习方法的几种观点"></a>关于学习方法的几种观点</h2><h3 id="天才派"><a href="#天才派" class="headerlink" title="天才派"></a>天才派</h3><p>有人认为：</p>
<blockquote>
<p>有些人天生就是神童，学什么都快．</p>
</blockquote>
<p>不否认人和人生来是有差距的．但成大事者很少是少年天才的，因为天才很多就伤仲永了．很多成功者都是经历了人生的磨练而没有倒下的人，这需要一种韧劲．天才不可多得，你我普通人需要的是持续的学习．</p>
<h3 id="后天努力派"><a href="#后天努力派" class="headerlink" title="后天努力派"></a>后天努力派</h3><p>有些人，认定：</p>
<blockquote>
<p>人定胜天，什么都是后天可以改变的．</p>
</blockquote>
<p>还是请承认，人生而有差距吧．</p>
<h3 id="方法派"><a href="#方法派" class="headerlink" title="方法派"></a>方法派</h3><p>我持一种观点，不知道是不是中庸思想的精髓：</p>
<blockquote>
<p>尽人事，听天命．</p>
</blockquote>
<p>这句话我最早在台湾大学吕世浩老师的<a href="http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/103S116" target="_blank" rel="noopener">&lt;史记&gt;</a>课程中听到的．把事情，自己认可的事情做到最好，做到极致；然后听天由命．做的过程中要讲求方法，找到最好的方法，最极致的方法．</p>
<p>读书或者学习需要讲求方法．我们可以采用主动学习的方式，比如跟人互动，给人讲等方式．这个可以参考<a href="https://www.jianshu.com/p/2d6f749b9f0d" target="_blank" rel="noopener">学习金字塔</a> ．虽然有很多人认为这个不是真的，但之所以流传这么广，还是有它的道理在的．</p>
<h2 id="如何迅速的学习新东西？我的观点"><a href="#如何迅速的学习新东西？我的观点" class="headerlink" title="如何迅速的学习新东西？我的观点"></a>如何迅速的学习新东西？我的观点</h2><h3 id="如何利用时间"><a href="#如何利用时间" class="headerlink" title="如何利用时间"></a>如何利用时间</h3><p>一定要有一个整片的时间学习，45分钟最好，不被打扰，全新全意．然后休息一会，然后继续．当然很多人是会被各种事情，会议等等打扰的．这个就看你自己的安排了．然后要利用好碎片时间，做好零碎化的阅读．</p>
<h3 id="如何高效学习？"><a href="#如何高效学习？" class="headerlink" title="如何高效学习？"></a>如何高效学习？</h3><p>主动的学．比如拿到一本书，先看大纲，知道要讲什么．然后阅读重点，然后实践重点．然后尝试去写出大概，或者能找到一个人跟他讲一遍也可．</p>
<p>读书这件事，建议可以先读一下&lt;如何阅读一本书&gt;．我的想法有点异想天开，因为毕竟不同类型的书还是需要不同的读法．</p>
<h3 id="如何坚持并训练自己"><a href="#如何坚持并训练自己" class="headerlink" title="如何坚持并训练自己"></a>如何坚持并训练自己</h3><p>自律的人才自由，健身学习亦然．有些事情需要坚持的．</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这是一篇给自己，也是给大家的鸡汤，希望大家喝完还是能吸收到一些营养的．需要做事，主动并快，这是我从互联网中学到的．</p>
]]></content>
        
        <categories>
            
            <category> soup </category>
            
            <category> learn </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 学习 </tag>
            
            <tag> 观点 </tag>
            
        </tags>
        
    </entry>
    
    
    
</search>
