<!doctype html>
<html>
<head>
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable"  content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no">
    
    
    <!--Simple SEO-->


<meta name="robots" content=all />
<meta name="google" content=all />
<meta name="googlebot" content=all />
<meta name="verify" content=all />
    <!--Title-->

<title>Spark聚合操作 | 时光咖啡馆</title>

<link rel="alternate" href="/atom.xml" title="时光咖啡馆" type="application/atom+xml">


<link rel="icon" href="/favicon.ico">

    
<link rel="stylesheet" href="/css/base.css">
<link rel="stylesheet" href="/css/pages/post.css">
<link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/thirdParty/highlight/github.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

    <!--script-->


<script src="//cdn.jsdelivr.net/npm/leancloud-storage@3.15.0/dist/av-min.js"></script>
<script>
  var appId = "PzYuF7A5UTlSlKpxASsstsxg-gzGzoHsz";
  var appKey = "HGSltyQPNS2hTy7C3qnRSO8a";
  var region = "";
  var serverURLs = "https://pzyuf7a5.lc-cn-n1-shared.com";
  AV.init({
    appId: appId,
    appKey: appKey,
    region: region,
    serverURLs: serverURLs
  });
</script>


<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>

<script>
  var admin = "wbgentleman";
  admin = admin.split(",");
  var gitalk = new Gitalk({
    clientID: "4542fd0f6a578529cf33",
    clientSecret: "09b02505b521d351a26ec490d7e2b81218774abf",
    repo: "honeyyouyoutalk",
    owner: "wbgentleman",
    admin: admin,
    id: decodeURIComponent(location.pathname),
    distractionFreeMode: false
  })

</script>

    
    
</head>

<body id="normal">
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<style>
    header{ top: 71px; position: absolute!important;}
    #container{padding-top: 151px!important;}
</style>
<div style="position:fixed;z-index:9999;left:0;top:0;width:100%;height:70px;background-color:#e0e0e0;color:#396CA5;border-bottom:1px solid #cecece;text-align:center;line-height:70px;white-space: nowrap;overflow: hidden;text-overflow: ellipsis">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<div id="wrap">
    <header  style="position: absolute;" >
    <div id="site-meta">
        <a href="/" id="logo">
            <h1 class="title">时光咖啡馆</h1>
        </a>
        
        <h2 class="subtitle">honeyyouyou</h2>
        
    </div>
    <ul id="nav">
        
            <li><a href="/"><i class="fa fa-home"></i>首页</a></li>
        
            <li><a href="/atom.xml"><i class="fa fa-rss"></i>RSS</a></li>
        
        <li id="search"><a href="javascript:void(0)"><i class="fa fa-search"></i>搜索</a></li>
    </ul>
</header>

    <div id="container">
        
<ul id="sidebar">
    
    
<li class="widget notification">
    <i class="fa fa-bell-o"></i>
    <div>
        
<p>记录工作和生活，做些总结和反思．愿您加入讨论．从事原创，支持原创．</p>
    </div>
</li>

    
    
<li class="widget widget-normal category">
    <h3 class="fa fa-th widget-title">分类</h3>
    <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/learn/"><i class="fa" aria-hidden="true">learn</i></a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/learn/code/"><i class="fa" aria-hidden="true">code</i></a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/learn/code/spark/"><i class="fa" aria-hidden="true">spark</i></a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/learn/principle/"><i class="fa" aria-hidden="true">principle</i></a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/learn/principle/spark/"><i class="fa" aria-hidden="true">spark</i></a><span class="category-list-count">1</span></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/soup/"><i class="fa" aria-hidden="true">soup</i></a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/soup/learn/"><i class="fa" aria-hidden="true">learn</i></a><span class="category-list-count">1</span></li></ul></li></ul>
</li>


    
    
<li class="widget widget-normal archive">
  <h3 class="fa fa-archive widget-title">归档</h3>
    <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/"><i class="fa" aria-hidden="true">十一月 2019</i></a><span class="archive-list-count">3</span></li></ul>
</li>


    
    
<li class="widget widget-normal popular-posts" id="popular-posts">
    <h3 class="fa fa-thermometer-3 widget-title">热门文章</h3>
    <ul id="popular-content">
        <li class="load-first"><i class="fa fa-spinner fa-pulse"></i></li>
    </ul>
</li>

    
    
<li class="widget widget-normal tags">
  <h3 class="fa fa-tags widget-title">标签云</h3>
  <div class="tagcloud-content">
    
      <a href="/tags/%E5%AD%A6%E4%B9%A0/" style="font-size: 0.2rem; color: #0a407c">学习</a> <a href="/tags/%E8%A7%82%E7%82%B9/" style="font-size: 0.14rem; color: #69c">观点</a> <a href="/tags/%E4%BB%A3%E7%A0%81/" style="font-size: 0.17rem; color: #386da4">代码</a> <a href="/tags/Spark/" style="font-size: 0.17rem; color: #386da4">Spark</a>
  </div>
</li>


    
    
<li class="widget widget-normal friends-link">
    <h3 class="fa fa-globe widget-title">友链</h3><br/>

    
        <a href="https://www.guokr.com/" class="fa" target="_blank">果壳科技</a>

    
        <a href="http://ocw.aca.ntu.edu.tw/ntu-ocw/" class="fa" target="_blank">台大开放式课程</a>

    
        <a href="https://www.yangzhiping.com/" class="fa" target="_blank">阳志平网志</a>

    
        <a href="https://blog.yxwang.me/" class="fa" target="_blank">AiurZellux的博客</a>

    
        <a href="http://zyan.cc/category/12/1/2/" class="fa" target="_blank">张宴的博客</a>

    
        <a href="https://frank-lam.github.io/fullstack-tutorial/#/introduction" class="fa" target="_blank">全栈工程师</a>

    
        <a href="http://geekaholiclin.github.io" class="fa" target="_blank">主题链接</a>

    

</li>

    
</ul>


        <div id="main">
    <article id="post">
        <div id="post-header">

            <h1 id="Spark聚合操作">
                
                Spark聚合操作
                
            </h1>
            <div class="article-meta">
    
    
    <span class="categories-meta fa-wrap">
            <i class="fa fa-folder-open-o"></i>
        <span>spark</span>
    </span>
    
    
    <span class="fa-wrap">
         <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
            学习
            
        </span>
    </span>
    
    
    <span class="fa-wrap">
        <i class="fa fa-clock-o"></i>
        <span class="date-meta ">2019/11/19</span>
    </span>
    
    
    <span class="fa-wrap">
            <i class="fa fa-thermometer-three-quarters"></i>
        <span class="hits hits-meta " data-leadcloud-title="Spark聚合操作"
              data-leadcloud-url="/2019/11/19/spark_aggregate_operations/"><i class="fa fa-spinner fa-spin"></i></span>
    </span>
    
    
</div>

            
            
        </div>
        
        <div id="post-body">
            <h1 id="Spark聚合操作-Spark-Aggregate-operations"><a href="#Spark聚合操作-Spark-Aggregate-operations" class="headerlink" title="Spark聚合操作 Spark Aggregate operations"></a>Spark聚合操作 Spark Aggregate operations</h1><h2 id="聚合操作简介"><a href="#聚合操作简介" class="headerlink" title="聚合操作简介"></a>聚合操作简介</h2><p>聚合操作，在sql，在编程概念中，或者说在人类思想概念中都是一个很重要的东西。<br>这个思想从哪里来的呢？应该是类推，演绎的继续。我们拆箱子，想看看每个箱子里面都多少个玩具，然后想知道每个玩具里面有多少个零件，想知道每个零件有几个齿轮轮毂，每个轮毂有多少道痕，每个痕里有几个细菌，每个细菌中有多少分子，到原子，到夸克就到了我们人类知道的最小的粒子了（不算时间，能量能虚幻之物）。然后往上，我们可以回滚。也就有了基本的聚合概念。每个屋里有多少箱子，每个城市有多少个屋，每个国家多少个城市，每个星球多少个国家，每个星系多少个星球，每个宇宙多少个星系。<br>还是回到讨论的聚合概念中来。有了刚才的类比，就比较好想聚合的概念了。在手中拥有的数据中，可以通过任意维度进行聚合，聚合后又可以查询原始数据。这就是作为的上钻和下钻的概念。<br>在sql中如何进行上下钻，切片等操作呢？</p>
<h2 id="Spark中上卷操作"><a href="#Spark中上卷操作" class="headerlink" title="Spark中上卷操作"></a>Spark中上卷操作</h2><p>这个需要分RDD和Dataframe来讲，因为这个的存储方式不一样，所以需要分开讲。</p>
<h3 id="Spark中RDD的上卷"><a href="#Spark中RDD的上卷" class="headerlink" title="Spark中RDD的上卷"></a>Spark中RDD的上卷</h3><p>RDD中首先将数据分为两类，list类型的map类型的。可以理解为一维和二维的数据。</p>
<h4 id="一维数据的操作"><a href="#一维数据的操作" class="headerlink" title="一维数据的操作"></a>一维数据的操作</h4><p>RDD中一维数据的上卷操作主要有一个。</p>
<ul>
<li><p>reduce<br>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.sparkContext.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b)</span><br><span class="line"><span class="keyword">val</span> result:<span class="type">Int</span> = rdd.reduce(func)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>结果：6</p>
</li>
<li><p>aggregate<br>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.sparkContext.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b)</span><br><span class="line"><span class="keyword">val</span> result:<span class="type">Int</span> = rdd.aggregate(<span class="number">0</span>)(func,func)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>结果：6</p>
</li>
</ul>
<h4 id="二维数据的操作"><a href="#二维数据的操作" class="headerlink" title="二维数据的操作"></a>二维数据的操作</h4><p>RDD中二维数据的上卷操作主要有以下几个。</p>
<ol>
<li>reduceByKey<br>此处举两个例子：</li>
</ol>
<ul>
<li><p>第一个使用网上的例子</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.sparkContext.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">2</span>),(<span class="number">3</span>,<span class="number">4</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; &#123;a+b&#125;</span><br><span class="line"><span class="keyword">val</span> result = rdd.reduceByKey(func)</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<p>结果: (1,2),(3,10)</p>
</li>
<li><p>第二个使用经典的例子，wordcount<br>写测试文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -e <span class="string">"a b c\nb a c\na d c b e\nb c a d\nc e f f"</span> &gt; /tmp/wc.log</span><br></pre></td></tr></table></figure>
<p>统计分析</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; &#123;a+b&#125;</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).reduceByKey(func)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>扩展 WordCount使用不同的聚合函数</p>
<ol>
<li>groupByKey<br>直接使用groupByKey的wordcount例子。Dataset版。<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>)</span><br><span class="line"><span class="keyword">val</span> func = (a:(<span class="type">String</span>,<span class="type">Int</span>),b:(<span class="type">String</span>,<span class="type">Int</span>)) =&gt; (a._1,a._2+b._2)</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).groupByKey(_._1).reduceGroups(func).map(_._2)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>直接使用groupByKey的wordcount例子。RDD版。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="keyword">val</span> func = (a:(<span class="type">String</span>,<span class="type">Int</span>),b:(<span class="type">String</span>,<span class="type">Int</span>)) =&gt; (a._1,a._2+b._2)</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).groupByKey(_._1).reduceGroups(func).map(_._2)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>
<p>直接使用groupByKey的wordcount例子。RDD获取长度版。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).groupByKey().mapValues(_.size)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><p>foldByKey</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; &#123;a+b&#125;</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).foldByKey(<span class="number">0</span>)(func)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>
</li>
<li><p>aggregateByKey</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="keyword">val</span> func = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; &#123;a+b&#125;</span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).aggregateByKey(<span class="number">0</span>)(func,func)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>
</li>
<li><p>combineByKey</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = spark.read.textFile(<span class="string">"file:///tmp/wc.log"</span>).rdd</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createCombiner</span> </span>= (x : <span class="type">Int</span>) =&gt; x</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergeValue</span> </span>= (acc1: <span class="type">Int</span>, x:<span class="type">Int</span> ) =&gt; acc1 + x</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergeCombiner</span> </span>= (acc1:<span class="type">Int</span>, acc2:<span class="type">Int</span>) =&gt; acc1 + acc2</span><br><span class="line"><span class="comment">//val result = rdd.flatMap( line =&gt; &#123;for(ele &lt;- line.split(" ")) yield (ele,1)&#125;).combineByKey( (x:Int) =&gt; x  , (acc1:Int, x:Int ) =&gt; acc1 + x  , ( acc1:Int , acc2 :Int ) =&gt; acc1 + acc2  )</span></span><br><span class="line"><span class="keyword">val</span> result = rdd.flatMap( line =&gt; &#123;<span class="keyword">for</span>(ele &lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (ele,<span class="number">1</span>)&#125;).combineByKey(createCombiner,mergeValue,mergeCombiner)</span><br><span class="line">result.collect.foreach(println)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="Spark中DataFrame的上卷"><a href="#Spark中DataFrame的上卷" class="headerlink" title="Spark中DataFrame的上卷"></a>Spark中DataFrame的上卷</h3><p>DataFrame可以认为是包含Schema的RDD。它的上卷没有一维二维之分。</p>
<h4 id="GroupBy"><a href="#GroupBy" class="headerlink" title="GroupBy"></a>GroupBy</h4><ol>
<li><p>Build df example method1</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(</span><br><span class="line">  <span class="type">Seq</span>(</span><br><span class="line">    (<span class="string">"first"</span>, <span class="type">Array</span>(<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">2.1</span>, <span class="number">5.4</span>)),</span><br><span class="line">    (<span class="string">"test"</span>, <span class="type">Array</span>(<span class="number">1.5</span>, <span class="number">0.5</span>, <span class="number">0.9</span>, <span class="number">3.7</span>)),</span><br><span class="line">    (<span class="string">"choose"</span>, <span class="type">Array</span>(<span class="number">8.0</span>, <span class="number">2.9</span>, <span class="number">9.1</span>, <span class="number">2.5</span>))</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> dfWithoutSchema = spark.createDataFrame(rdd)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Build df example method2</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(</span><br><span class="line">  <span class="type">Seq</span>(</span><br><span class="line">    (<span class="string">"first"</span>, <span class="type">Array</span>(<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">2.1</span>, <span class="number">5.4</span>)),</span><br><span class="line">    (<span class="string">"test"</span>, <span class="type">Array</span>(<span class="number">1.5</span>, <span class="number">0.5</span>, <span class="number">0.9</span>, <span class="number">3.7</span>)),</span><br><span class="line">    (<span class="string">"choose"</span>, <span class="type">Array</span>(<span class="number">8.0</span>, <span class="number">2.9</span>, <span class="number">9.1</span>, <span class="number">2.5</span>))</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> dfWithSchema = spark.createDataFrame(rdd).toDF(<span class="string">"id"</span>, <span class="string">"vals"</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>Build df example method3</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">val</span> rowsRdd: <span class="type">RDD</span>[<span class="type">Row</span>] = sc.parallelize(</span><br><span class="line">  <span class="type">Seq</span>(</span><br><span class="line">    <span class="type">Row</span>(<span class="string">"first"</span>, <span class="number">2.0</span>, <span class="number">7.0</span>),</span><br><span class="line">    <span class="type">Row</span>(<span class="string">"second"</span>, <span class="number">3.5</span>, <span class="number">2.5</span>),</span><br><span class="line">    <span class="type">Row</span>(<span class="string">"third"</span>, <span class="number">7.0</span>, <span class="number">5.9</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> schema = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)).add(<span class="type">StructField</span>(<span class="string">"val1"</span>, <span class="type">DoubleType</span>, <span class="literal">true</span>)).add(<span class="type">StructField</span>(<span class="string">"val2"</span>, <span class="type">DoubleType</span>, <span class="literal">true</span>))</span><br><span class="line"><span class="keyword">val</span> df = spark.createDataFrame(rowsRdd, schema)</span><br></pre></td></tr></table></figure></li>
<li><p>Build df example method4</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ELE</span>(<span class="params">id:<span class="type">String</span>,val1:<span class="type">Double</span>,val2:<span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rdd</span> </span>= sc.parallelize(</span><br><span class="line">  <span class="type">Seq</span>(</span><br><span class="line">    (<span class="string">"first"</span>, <span class="number">2.0</span>, <span class="number">7.0</span>),</span><br><span class="line">    (<span class="string">"second"</span>, <span class="number">3.5</span>, <span class="number">2.5</span>),</span><br><span class="line">    (<span class="string">"third"</span>, <span class="number">7.0</span>, <span class="number">5.9</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> df = rdd.map(s =&gt; <span class="type">ELE</span>(s._1,s._2,s._3)).toDF</span><br></pre></td></tr></table></figure>
</li>
<li><p>Do groupby on df</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.groupBy(<span class="string">"id"</span>).agg(sum(<span class="string">"val1"</span>).as(<span class="string">"total"</span>)).show(<span class="literal">false</span>)</span><br><span class="line">df.groupBy().agg(sum(<span class="string">"val1"</span>).as(<span class="string">"total"</span>)).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="Partition-By"><a href="#Partition-By" class="headerlink" title="Partition By"></a>Partition By</h4><p>参考另一篇文章&lt;Hive 窗口函数测试&gt;</p>
<h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><ul>
<li>参考：<a href="https://stackoverflow.com/questions/36447057/spark-group-multiple-rdd-items-by-key" target="_blank" rel="noopener">ReduceByKeyAndGroupByKey</a></li>
<li>参考：<a href="https://gerardnico.com/db/spark/rdd/reduce" target="_blank" rel="noopener">ReduceAndReduceByKey</a></li>
<li>参考：<a href="https://blog.csdn.net/u013514928/article/details/56680825" target="_blank" rel="noopener">Aggregate与AggregateByKey</a></li>
<li>参考：<a href="https://backtobazics.com/big-data/spark/apache-spark-aggregatebykey-example/" target="_blank" rel="noopener">Aggregate实例</a></li>
<li>参考：<a href="https://medium.com/@yesilliali/apache-spark-understanding-zerovalue-in-aggregatebykey-function-3d7df62567ae" target="_blank" rel="noopener">理解AggregateByKey中的Zero值</a></li>
<li>参考：<a href="https://dev.to/yashwanth2804/different-ways-to-word-count-in-apache-spark-1193" target="_blank" rel="noopener">DifferentWaysToWordCountInApacheSpark</a></li>
<li>参考：<a href="https://github.com/vaquarkhan/Apache-Kafka-poc-and-notes/wiki/reducebykey-vs-combinebykey-Apache-Spark" target="_blank" rel="noopener">CombineByKey</a></li>
<li>参考：<a href="https://backtobazics.com/big-data/apache-spark-combinebykey-example/" target="_blank" rel="noopener">CombineByKey2</a></li>
<li>参考：<a href="https://docs.databricks.com/spark/latest/spark-sql/udaf-scala.html" target="_blank" rel="noopener">UDAF实例</a></li>
<li>参考：<a href="https://stackoverflow.com/questions/37440373/spark-dataframe-aggregate-column-values-by-key-into-list" target="_blank" rel="noopener">DataFrame中Concate操作</a></li>
<li>参考：<a href="https://stackoverflow.com/questions/29383578/how-to-convert-rdd-object-to-dataframe-in-spark" target="_blank" rel="noopener">RDDToDataframe</a></li>
<li>参考：<a href="https://indatalabs.com/blog/convert-spark-rdd-to-dataframe-dataset" target="_blank" rel="noopener">RDDToDFAndDataSet</a></li>
</ul>

        </div>
        <div id="post-footer">
            <div class="avatar" >
                <img src="/img/meihouwang.jpeg" alt="avatar"/>
                
                <a href="javascript:void(0)" class="high-song">high起来 &#128541;</a>
                
                
                <a href="javascript:void(0)" class="donate fa">赠我一杯 &#128536;</a>
                
            </div>
            <ul class="author-profile-section">
                <li>
                  
                  作者:
                  
                    
                    <a href="/about.html">Robin</a>
                </li>
                
                <li>发表日期: <span>2019-11-19  21:49:10</span></li>
                
                <li>最后编辑日期: <span>2019-11-21  11:21:39</span></li>
                
                <li class="post-category">
                    文章分类:
                    
                    <a href="/categories/learn/">learn</a>
                    
                    <a href="/categories/learn/principle/">principle</a>
                    
                    <a href="/categories/learn/principle/spark/">spark</a>
                    
                </li>
                <li class="post-tags">
                    文章标签:
                    
                    <a href="/tags/%E5%AD%A6%E4%B9%A0/">学习</a>
                    
                    <a href="/tags/%E4%BB%A3%E7%A0%81/">代码</a>
                    
                    <a href="/tags/Spark/">Spark</a>
                    
                </li>
                
                <li> 版权声明: <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank">
知识共享署名-非商业性使用-禁止演绎 3.0 未本地化版本许可协议（CC BY-NC-ND 3.0）
</a></li>
                
            </ul>
            <div id="donate-wrap">
                
                
                
                <img src="/img/alipay.jpg" alt="支付宝付款" class="donate-img">
                
                
            </div>
        </div>
    </article>
    <div class="article-nav">
        
        
        <a href="/2019/11/14/spark_analysis_001/" class="next-post fa">Spark源码分析001</a>
        
    </div>
    
    <div id="comments">
        

<script>
  gitalk.render("comments");
</script>



    </div>
    
</div>


    </div>
    <footer id="footer">
    
    <div class="social">
        
        <a href="https://www.freecodecamp.org/" class="fa fa-free-code-camp" target="_blank" title="freecodecamp"></a>
        
        <a href="https://github.com/robinwang7" class="fa fa-github" target="_blank" title="Follow me~"></a>
        
        <a href="mailto:313773543@qq.com" class="fa fa-envelope" target="_blank" title="email me~"></a>
        
    </div>
    
    <div>
        
        <a href="/" class="copyright-links">Robin</a>&copy;2015 - 2019.All Rights
        Reserved.
    </div>
    <p>Powered by <a href="https://hexo.io" class="copyright-links" target="_blank">Hexo</a> 
    </p>
    
    
    <p>
        <span id="busuanzi_container_site_uv" class="fa fa-bar-chart">
        欢迎第<span id="busuanzi_value_site_uv"><i class="fa fa-spinner fa-spin"></i></span>位小伙伴~
        </span>
    </p>
    
</footer>

</div>
    <ul id="tools">
    <li class="totop-btn fa fa-angle-up"></li>
    <li class="exchange-btn fa fa-exchange"></li>
  
    <li class="toc-btn fa fa-list-ul"></li>
    
    

    
    <li class="comment-btn fa fa-comments-o">
        <a href="#comments" title="comments"></a>
    </li>
    
</ul>
<p id="process"></p>
<div id="search-overlay">
    <div class="search-area-wrap">
        <div id="search-area">
            <div class="input-wrap focus">
                <i class="fa fa-search" aria-hidden="true"></i>
                <input id="search-input" autofocus autocomplete="off" type="text"
                       placeholder="search this website..."/>
            </div>
            <ul id="search-result">
                <li class="load-first"><i class="fa fa-spinner fa-pulse"></i></li>
            </ul>
        </div>
    </div>
</div>

    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark聚合操作-Spark-Aggregate-operations"><span class="toc-number">1.</span> <span class="toc-text">Spark聚合操作 Spark Aggregate operations</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#聚合操作简介"><span class="toc-number">1.1.</span> <span class="toc-text">聚合操作简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark中上卷操作"><span class="toc-number">1.2.</span> <span class="toc-text">Spark中上卷操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark中RDD的上卷"><span class="toc-number">1.2.1.</span> <span class="toc-text">Spark中RDD的上卷</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#一维数据的操作"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">一维数据的操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#二维数据的操作"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">二维数据的操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark中DataFrame的上卷"><span class="toc-number">1.2.2.</span> <span class="toc-text">Spark中DataFrame的上卷</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GroupBy"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">GroupBy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Partition-By"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">Partition By</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考："><span class="toc-number">1.3.</span> <span class="toc-text">参考：</span></a></li></ol></li></ol>


    <script src="/js/highsong.js"></script>



<script src="/js/search.js"></script>
<script type="text/javascript">
    //theme config datas
    var copyrightObj = {};
    copyrightObj.enable = 'true';
    copyrightObj.triggerCopyLength = '200';
    copyrightObj.appendText = '商业转载请联系作者获得授权,非商业转载请注明出处 © example';
    var leancloudObj = {};
    leancloudObj.enable = 'true';
    leancloudObj.className = 'blog_count';
    leancloudObj.limits = '10';
</script>
<script type="text/javascript">
    var search = {};
    var search_path = "search.xml";
    if (!search_path) {
        search_path = "search.xml";
    }
    search.path = "/" + search_path;
    search.func =  _ajax.init();
</script>
<script src="/js/app.js"></script>


</body>
</html>